Homework 3
================

``` r
library(tidyverse)
```

    ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(ggthemes)
```

Problem 1
---------

Load BRFSS data

``` r
data(brfss_smart2010)

brfss_smart2010 <- brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, state_and_county = locationdesc) %>% 
  filter(topic == "Overall Health", !is.na(response)) %>%
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
```

*In 2002, which states were observed at 7 locations?*

``` r
knitr::kable(brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  distinct(state_and_county) %>% 
  tally() %>% 
  filter(n == 7)
)
```

| state |    n|
|:------|----:|
| CT    |    7|
| FL    |    7|
| NC    |    7|

Connecticut, Florida, and North Carolina were observed at 7 distinct locations in 2002.

*Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.*

``` r
#Save number of locations data in each state
number_locations <- brfss_smart2010 %>% 
  group_by(state, year) %>% 
  distinct(state_and_county, year) %>% 
  add_tally()

left_join(brfss_smart2010, number_locations) %>% 
  rename(n_unique_locations = n) %>%
  distinct(year, state, n_unique_locations) %>% 
  ggplot(aes(x = year, y = n_unique_locations)) +
  geom_line(aes(color = state)) + 
  theme_bw() + 
  labs(
    title = "Locations Responding to BRFSS by State, 2002-2010",
    x = "Number of Locations",
    y = "Year",
    caption = "Data from brfss"
  ) + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
    )
```

    ## Joining, by = c("year", "state", "state_and_county")

![](p8105_hw3_lec2197_files/figure-markdown_github/spaghetti-1.png)

*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*

``` r
brfss_smart2010 %>%
  filter(response == "Excellent", year == 2002 | year == 2006 | year == 2010) %>% 
  group_by(state, year) %>% 
  summarize(mean = mean(data_value), sd = sd(data_value))
```

    ## # A tibble: 147 x 4
    ## # Groups:   state [?]
    ##    state  year  mean    sd
    ##    <chr> <int> <dbl> <dbl>
    ##  1 AK     2002  27.9 NA   
    ##  2 AL     2002  18.5 NA   
    ##  3 AL     2006  23.2 NA   
    ##  4 AL     2010  18.4  2.63
    ##  5 AR     2002  24.1 NA   
    ##  6 AR     2006  19.6  3.41
    ##  7 AR     2010  25.4  3.16
    ##  8 AZ     2002  24.1  3.54
    ##  9 AZ     2006  20.9  3.86
    ## 10 AZ     2010  21.6  5.24
    ## # ... with 137 more rows

*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.*

``` r
brfss_smart2010 %>%
  group_by(state, year, response) %>% 
  summarize(mean = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_point(aes(color = state), size = .1) +
  geom_line(aes(color = state), alpha = .3) + 
  facet_grid( ~ response)  +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Proportion of Response Types by State, 2002-2010",
    x = "Year",
    y = "Mean Proportion",
    caption = "Data from brfss"
  ) + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
    ) 
```

    ## Warning: Removed 21 rows containing missing values (geom_point).

    ## Warning: Removed 1 rows containing missing values (geom_path).

![](p8105_hw3_lec2197_files/figure-markdown_github/unnamed-chunk-1-1.png)

``` r
ggsave("brfss_plot.jpg", scale = 2)
```

    ## Saving 14 x 10 in image

    ## Warning: Removed 21 rows containing missing values (geom_point).

    ## Warning: Removed 1 rows containing missing values (geom_path).

That's a small version, so I'm printing a more-readable one below (from the saved plot):

``` r
knitr::include_graphics("./brfss_plot.jpg")
```

<img src="./brfss_plot.jpg" width="100%" />

Problem 2
---------

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called instacart).

``` r
data(instacart)
```

*The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illustrative examples of observations.*

This large dataset contains information from Instacart, an online grocery service. Each of the 1384617 observations is an individual product ordered in a given order, with associated contextual information about the following:

-   the user, such as the days since the user has made a prior order and their ID

-   order session info, such as the index of the order in which the product was added into the cart for the given order session, and the order session's hour of day and day of week

-   the product, such as its ID, aisle, and department.

The largest order in this dataset is 80 items (using add to cart order as a proxy)! The median time of order for individual products is 2 p.m. 21 departments are represented in the data, the most popular of which is produce. The least popular is the bulk goods department.

There are 134 aisles represented in the data, the most popular of which is fresh vegetables. The least popular is the beauty aisle.

*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*

``` r
instacart %>%
  group_by(department, aisle, aisle_id) %>% 
  tally() %>% 
  ungroup() %>% 
  mutate(aisle = factor(aisle, levels = aisle[order(n)])) %>% 
  ggplot(aes(x = aisle, y = n, fill = department)) +
  geom_col() + 
  coord_flip() +
  theme_bw() + 
  theme(axis.text.y = element_text(size = rel(0.5))) +
  labs(
    title = "Popular Aisles",
    x = "aisle name",
    y = "order count",
    caption = "Data from instacart"
  ) + 
  viridis::scale_color_viridis(
    name = "Department", 
    discrete = TRUE
    ) 
```

![](p8105_hw3_lec2197_files/figure-markdown_github/plot%20number%20items%20ordered%20in%20each%20aisle-1.png)

``` r
ggsave("instacart_plot.jpg", scale = 2)
```

    ## Saving 14 x 10 in image

That's a small version, so I'm printing a more-readable one below (from the saved plot):

``` r
knitr::include_graphics("./instacart_plot.jpg")
```

<img src="./instacart_plot.jpg" width="100%" />

*Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.*

``` r
baking <- instacart %>% 
  filter(aisle == "baking ingredients") %>% 
  summarize(most_popular = names(which.max(table(product_name))))

dog_food <- instacart %>% 
  filter(aisle == "dog food care") %>% 
  summarize(most_popular = names(which.max(table(product_name))))

veg_fruit <- instacart %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  summarize(most_popular = names(which.max(table(product_name))))

knitr::kable(rbind(baking, dog_food, veg_fruit) %>% 
  mutate(aisle = c("baking ingredients", "dog food care", "packaged vegetables fruits"))
)
```

| most\_popular                                 | aisle                      |
|:----------------------------------------------|:---------------------------|
| Light Brown Sugar                             | baking ingredients         |
| Snack Sticks Chicken & Rice Recipe Dog Treats | dog food care              |
| Organic Baby Spinach                          | packaged vegetables fruits |

*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*

``` r
instacart %>% 
    filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
    group_by(order_dow, product_name) %>% 
    summarize(mean_hour = mean(order_hour_of_day)) %>% 
    ungroup() %>% 
    arrange(product_name) %>% 
    spread(key = product_name, value = mean_hour) %>% 
    mutate(order_dow = replace(order_dow, 1:7, c("sunday", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday"))) %>% 
    rename('Order Day of Week' = order_dow) %>% 
  knitr::kable(caption = "Mean Hour of Day Order Placed (x/24h)")
```

| Order Day of Week |  Coffee Ice Cream|  Pink Lady Apples|
|:------------------|-----------------:|-----------------:|
| sunday            |          13.77419|          13.44118|
| monday            |          14.31579|          11.36000|
| tuesday           |          15.38095|          11.70213|
| wednesday         |          15.31818|          14.25000|
| thursday          |          15.21739|          11.55172|
| friday            |          12.26316|          12.78431|
| saturday          |          13.83333|          11.93750|

Problem 3
---------

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called ny\_noaa).

``` r
data(ny_noaa)
```

This dataset is composed of weather station data in NY state. The key variables are `prcp`, or precipitation; `snow`, or snowfall; `snwd`, or snow depth; and `tmax`/`tmin`, or maximum and minimum temperature. THe ID of the weather station as well as the date of data collection (data is day-level) is also present. This dataset has 2595176 observations, but a good deal of missing data. A table of missing data is below:

``` r
ny_noaa %>% 
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>% 
  mutate(percent_nulls = 100*num_nulls/nrow(ny_noaa)) %>% 
  knitr::kable()
```

| feature |  num\_nulls|  percent\_nulls|
|:--------|-----------:|---------------:|
| id      |           0|         0.00000|
| date    |           0|         0.00000|
| prcp    |      145838|         5.61958|
| snow    |      381221|        14.68960|
| snwd    |      591786|        22.80331|
| tmax    |     1134358|        43.71025|
| tmin    |     1134420|        43.71264|

Nearly half of the observations in the dataset have missing temperature data. Such a high null value might be a result of systematic lack of data collection rather than aberrations. Other abberations also seem to be present. Precipitation data is the most reliably present in the dataset. The max observed value for precipitation is 22860, and for snowfall is 10160. Both max values seem very high for precipitation; but checking the source for the data, the units of precipitation are in tenths of a millimeter and for snowfall are in millimeters.It also seems off that there would be a negative number for snowfall, although perhaps this represents measurement of snowmelt.

Similarly, the temperature data is in tenths of a degree Celsius. The temperature sensors are very sensitive, but possibly become less accurate on certain temperatures; we see similar maximum values for `tmax` and `tmin`. It looks like the "600" value, or 60ºC, is a ceiling, appearing 34 times in both variables.

``` r
ny_noaa %>% 
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>% 
  summarize(median_tmax = median(tmax, na.rm = TRUE),
            IQR_tmax = IQR(tmax, na.rm = TRUE),
            max_tmax = max(tmax, na.rm = TRUE),
            min_tmax = min(tmax, na.rm = TRUE),
            count_600 = sum(ifelse(tmax == 600, 1, 0), na.rm = TRUE)
              ) %>% 
  knitr::kable()
```

|  median\_tmax|  IQR\_tmax|  max\_tmax|  min\_tmax|  count\_600|
|-------------:|----------:|----------:|----------:|-----------:|
|           150|        183|        600|       -389|          34|

``` r
ny_noaa %>% 
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>% 
  summarize(median_tmin = median(tmin, na.rm = TRUE),
            IQR_tmin = IQR(tmin, na.rm = TRUE),
            max_tmin = max(tmin, na.rm = TRUE),
            min_tmin = min(tmin, na.rm = TRUE),
            count_600 = sum(ifelse(tmax == 600, 1, 0), na.rm = TRUE)
            ) %>% 
  knitr::kable()  
```

|  median\_tmin|  IQR\_tmin|  max\_tmin|  min\_tmin|  count\_600|
|-------------:|----------:|----------:|----------:|-----------:|
|            33|        150|        600|       -594|          34|

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

``` r
ny_noaa <- ny_noaa %>% 
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10, prcp = prcp/100, snow = snow/10, snwd = snwd/10 ) %>% 
  rename(tmax_ºC = tmax, tmin_ºC = tmin, prcp_cm = prcp , snow_cm = snow, snwd_cm = snwd)

head(sort(table(ny_noaa$snow_cm, useNA = "ifany"), decreasing = TRUE))
```

    ## 
    ##       0    <NA>     2.5     1.3     5.1     7.6 
    ## 2008508  381221   31022   23095   18274   10173

The most common observed value for snowfall is 0cm by a factor of ~65x, compared to the next non-missing value (2.5cm). This is expected, because the weather data is year-round.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
ny_noaa %>% 
  mutate(month = as.numeric(month)) %>% 
  filter(month == 1 | month == 7) %>%
  group_by(month, year, id) %>%
  summarize(mean_tmax = mean(tmax_ºC)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point() + 
  facet_grid(~month) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Average temperatures at NY weather stations, January and July ",
    x = "Year",
    y = "Maximum Temperature (ºC)",
    caption = "Data from noaa"
  ) 
```

    ## Warning: Removed 7058 rows containing missing values (geom_point).

![](p8105_hw3_lec2197_files/figure-markdown_github/two%20panel%20plot-1.png)

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
ny_noaa %>% 
  ggplot(aes(x = tmax_ºC, y = tmin_ºC)) +
  geom_smooth() + 
  theme_bw() + 
  labs(
    title = "Relationship of daily max to daily min temperatures at NY weather stations, 1981-2010",
    x = "Maximum Temperature (ºC)",
    y = "Minimum Temperature (ºC)",
    caption = "Data from noaa"
  ) 
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

    ## Warning: Removed 1136276 rows containing non-finite values (stat_smooth).

![](p8105_hw3_lec2197_files/figure-markdown_github/two%20panel%20tmax%20cis%20tmin-1.png)
